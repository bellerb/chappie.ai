{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 = $4\n",
      "player 2 = $0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Kuhn_Poker:\n",
    "    def __init__(self, p1 = 2, p2 = 2):\n",
    "        self.pot = 0\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.a1 = None\n",
    "        self.a2 = None\n",
    "        self.actions = ['check','raise','fold']\n",
    "        self.cards = {'king':3, 'queen':2, 'jack':1}\n",
    "        \n",
    "    def deal(self):\n",
    "        cards = list(self.cards.keys())\n",
    "        p1 = random.choice(cards)\n",
    "        cards.remove(p1)\n",
    "        p2 = random.choice(cards)\n",
    "        return p1, p2\n",
    "    \n",
    "    def place_bet(self, player, amount):\n",
    "        self.pot += amount\n",
    "        setattr(self, player, getattr(self, player) - amount)\n",
    "    \n",
    "    def choose_action(self, player, action, amount = 1):\n",
    "        if isinstance(action, int):\n",
    "            setattr(self, f'a{player[-1]}', self.actions[action])\n",
    "        elif str(action).lower() in self.actions:\n",
    "            setattr(self, f'a{player[-1]}', str(action).lower())\n",
    "        else:\n",
    "            return False\n",
    "        if getattr(self, f'a{player[-1]}') == 'raise' or getattr(self, f'a{player[-1]}') == 'call':\n",
    "            game.place_bet(player, amount)\n",
    "        return True\n",
    "    \n",
    "    def pay_out(self, p1, p2):\n",
    "        if self.cards[p1] > self.cards[p2]:\n",
    "            self.p1 += self.pot\n",
    "        else:\n",
    "            self.p2 += self.pot\n",
    "        self.pot = 0\n",
    "        \n",
    "game = Kuhn_Poker()\n",
    "\n",
    "#Anti up\n",
    "game.place_bet('p1', 1)\n",
    "game.place_bet('p2', 1)\n",
    "\n",
    "#Deal cards\n",
    "p1, p2 = game.deal()\n",
    "\n",
    "#Choose action\n",
    "game.choose_action('p1', 'Check')\n",
    "game.choose_action('p2', 'Raise')\n",
    "game.choose_action('p1', 'Call')\n",
    "\n",
    "game.pay_out(p1, p2)\n",
    "print(f'player 1 = ${game.p1}\\nplayer 2 = ${game.p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash Equilibrium (Aproximation)\n",
    "$u_{1}(\\sigma) + \\epsilon \\geq \\underset{\\sigma^{'}_{1}\\epsilon \\sum_{1}}{\\text{max}}u_{1}(\\sigma^{'}_{1},\\sigma_{2})$ <br>\n",
    "$u_{2}(\\sigma) + \\epsilon \\geq \\underset{\\sigma^{'}_{2}\\epsilon \\sum_{2}}{\\text{max}}u_{2}(\\sigma^{'}_{2},\\sigma_{2})$\n",
    "\n",
    "$u=\\text{utility function}$ <br>\n",
    "$\\sigma=\\text{player strategy}$\n",
    "\n",
    "What this means is that both players are playing to win, meaning they are each playing their best reponces to the players strategy. Because both players are trying to win this makes the best each player can do is tie. Nash equilibrium has an exploitability of zero meaning it is theoretically a unbeatable strategy making it optimal for most games.\n",
    "\n",
    "## Regret-Matching+\n",
    "$r(s,a)=v_{i}(s,a)-\\sum_{b \\epsilon A(s)}{p(s,b)v_{i}(s,b)}$\n",
    "\n",
    "$r=\\text{beliefs (range)}$ <br>\n",
    "$v=\\text{counterfactual value}$ <br>\n",
    "$p=\\text{player policy}$ <br>\n",
    "$s=\\text{public state}$\n",
    "\n",
    "$Q^{t}(s,a)=\\left(Q^{t-1}(s,a)+r^{t}(s,a)\\right)^{+}$\n",
    "\n",
    "$Q=\\text{regret like value}$ <br>\n",
    "$s=\\text{public state}$ <br>\n",
    "$a=\\text{action}$ <br>\n",
    "$r=\\text{beliefs (range)}$ <br>\n",
    "$t=\\text{time step}$\n",
    "\n",
    "$\\pi^{t+1}(s,a)=\\frac{Q^{t}(s,a)}{\\sum_{b}{Q^{t}(s,b)}}$\n",
    "\n",
    "$\\pi=\\text{player strategy}$ <br>\n",
    "$s=\\text{public state}$ <br>\n",
    "$Q=\\text{regret like value}$\n",
    "\n",
    "## Linearly weighted policy averaging\n",
    "\n",
    "$\\bar{\\pi}^{T}_{i}=\\frac{2}{(T^{2}+T)\\sum^{T}_{t=1}{t\\pi^{t}_{i}}}$\n",
    "\n",
    "$\\pi=\\text{player strategy}$ <br>\n",
    "$i=\\text{player}$ <br>\n",
    "$T=\\text{Iterations}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class GT_CFR:\n",
    "    def __init__(self, nn, s, c, action_space, k = 1):\n",
    "        self.T = 0\n",
    "        self.k = k #Total actions to expand\n",
    "        self.s = s #Total expansion simulations\n",
    "        self.c = c #Expansion simulations per regret update\n",
    "        self.H = [] #History\n",
    "        self.L = {} #Tree\n",
    "        self.action_space = action_space\n",
    "        self.CVPN = nn\n",
    "        \n",
    "    class Node:\n",
    "        def __init__(self):\n",
    "            self.N = 0\n",
    "            self.Q = []\n",
    "            self.v = []\n",
    "            self.p = []\n",
    "            self.r = []\n",
    "            self.CFR_pi = []\n",
    "            self.pUCT_pi = []\n",
    "            self.actions = []\n",
    "        \n",
    "    def state_hash(self, s):\n",
    "        result = hash(str(s))\n",
    "        return result\n",
    "    \n",
    "    def CFR(self, s, Q_1 = 0):\n",
    "        s_hash = self.state_hash(s)\n",
    "        if len(self.L[s_hash].actions) == 0: \n",
    "            B = (s, self.L[s_hash].r)\n",
    "            v, p = self.CVPN.predict(B)\n",
    "            self.L[s_hash].v = v\n",
    "            self.L[s_hash].p = p\n",
    "\n",
    "        r_sum = sum([v * p for v, p in zip(self.L[s_hash].v, self.L[s_hash].p)])\n",
    "        for i, v in enumerate(self.L[s_hash].v):\n",
    "            r = v - r_sum\n",
    "            self.L[s_hash].r.append(r)\n",
    "            if len(self.L[s_hash].Q) == len(self.L[s_hash].v):\n",
    "                self.L[s_hash].Q[i] = max(Q_1 + r, 0)\n",
    "            else:\n",
    "                self.L[s_hash].Q.append(max(Q_1 + r, 0))\n",
    "            \n",
    "        if len(self.L[s_hash].actions) > 0:\n",
    "            self.regret_matching_plus(s_hash)\n",
    "            for i, Q_1 in enumerate(self.L[s_hash].Q):\n",
    "                self.CFR(self.L[s_hash].actions[i], Q_1 = Q_1)\n",
    "                \n",
    "    def regret_matching_plus(self, s_hash):\n",
    "        #self.L[s_hash].CFR_pi = []\n",
    "        Q_sum = sum(self.L[s_hash].Q)\n",
    "        for i, Q in enumerate(self.L[s_hash].Q):\n",
    "            if Q_sum > 0:\n",
    "                self.L[s_hash].p[i] = Q / Q_sum\n",
    "            else:\n",
    "                self.L[s_hash].p[i] = 0\n",
    "            '''\n",
    "            if Q_sum > 0:\n",
    "                self.L[s_hash].CFR_pi.append(Q / Q_sum)\n",
    "            else:\n",
    "                self.L[s_hash].CFR_pi.append(0)\n",
    "            '''\n",
    "    \n",
    "    def expansion(self, s):\n",
    "        s_hash = self.state_hash(s)\n",
    "        if len(self.L[s_hash].actions) > 0:\n",
    "            self.regret_matching_plus(s_hash)\n",
    "            #self.pUCT(s_hash)\n",
    "            pi_bank = []\n",
    "            for rm_plus in self.L[s_hash].p:\n",
    "            #for rm_plus, pUCT in zip(self.L[s_hash].p, self.L[s_hash].pUCT_pi):\n",
    "                pi = rm_plus\n",
    "                #pi = (.5 * rm_plus) + (.5 * pUCT)\n",
    "                pi_bank.append(pi)\n",
    "            m_pi = max(pi_bank)\n",
    "            a_bank = [i for i, v in enumerate(pi_bank) if v == m_pi]\n",
    "            a = random.choice(a_bank)\n",
    "            self.expansion(self.L[s_hash].actions[a])\n",
    "        else:\n",
    "            #ADD ACTIONS TO TREE\n",
    "            for a in range(self.action_space):\n",
    "                a_hold = deepcopy(s)\n",
    "                if a == 1 and a_hold[0] == 1:\n",
    "                    a_hold[-2] += 1\n",
    "                elif a == 2 and a_hold[0] == 1:\n",
    "                    a_hold[-1] += a_hold[-2]\n",
    "                    a_hold[-2] = 0\n",
    "                elif (a == 0 or a == 1) and a_hold[0] == -1:\n",
    "                    a_hold[2] = 3 if a_hold[1] == 2 else 2\n",
    "                elif a == 1:\n",
    "                    a_hold[-1] += 1\n",
    "                elif a == 2:\n",
    "                    a_hold[-2] += a_hold[-1]\n",
    "                    a_hold[-1] = 0\n",
    "                a_hold[0] *= -1\n",
    "                search.L[s_hash].actions.append(a_hold)\n",
    "                a_hash = self.state_hash(a_hold)\n",
    "                search.L[a_hash] = search.Node()\n",
    "        self.L[s_hash].N += 1 #Update node visit count\n",
    "    \n",
    "    def GT_CFR(self, I):\n",
    "        for _ in range(int(self.s / self.c)):\n",
    "            self.CFR(I) #Run CFR\n",
    "            for _ in range(self.c):\n",
    "                self.expansion(I) #Grow tree\n",
    "\n",
    "class CVPN:\n",
    "    def predict(self, B):\n",
    "        s, r = B\n",
    "        c = max(s)\n",
    "        if c == 3:\n",
    "            v = [0.5, 0.5, 0]\n",
    "            p = [0.25, 0.25, 0.25]\n",
    "        elif c == 2:\n",
    "            v = [0.25, 0.25, 0.25]\n",
    "            p = [0.25, 0.25, 0.25]\n",
    "        else:\n",
    "            v = [0.5, 0, 0.5]\n",
    "            p = [0.25, 0.25, 0.25]\n",
    "        return v, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king queen\n",
      "[[-1, 3, -1, 1, 1], [-1, 3, -1, 2, 1], [-1, 3, -1, 0, 2]]\n",
      "[0.25, 0.25, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#Choose action\\ngame.choose_action('p1', 'Check')\\ngame.choose_action('p2', 'Raise')\\ngame.choose_action('p1', 'Call')\\n\\ngame.pay_out(p1, p2)\\nprint(f'player 1 = ${game.p1}\\nplayer 2 = ${game.p2}')\\n\""
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = CVPN()\n",
    "search = GT_CFR(nn, 2, 1, 3)\n",
    "\n",
    "game = Kuhn_Poker()\n",
    "\n",
    "#Anti up\n",
    "game.place_bet('p1', 1)\n",
    "game.place_bet('p2', 1)\n",
    "\n",
    "#Deal cards\n",
    "p1, p2 = game.deal()\n",
    "p1 = 'king'\n",
    "p2 = 'queen'\n",
    "\n",
    "print(p1, p2)\n",
    "s = [1] + [game.cards[p1]] + [-1, 1, 1]\n",
    "s_hash = search.state_hash(s)\n",
    "search.L[s_hash] = search.Node()\n",
    "search.L[s_hash].r = [1/(len(game.cards) - 1) if c != p1 else 0 for c in game.cards]\n",
    "\n",
    "search.GT_CFR(s)\n",
    "\n",
    "print(search.L[s_hash].actions)\n",
    "print(search.L[s_hash].Q)\n",
    "'''\n",
    "#Choose action\n",
    "game.choose_action('p1', 'Check')\n",
    "game.choose_action('p2', 'Raise')\n",
    "game.choose_action('p1', 'Call')\n",
    "\n",
    "game.pay_out(p1, p2)\n",
    "print(f'player 1 = ${game.p1}\\nplayer 2 = ${game.p2}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average overall Regret\n",
    "$R^{T}_{i}=\\frac{1}{T} \\underset{\\sigma^{*}_{i}\\epsilon \\sum_{i}}{\\text{max}} \\sum^{T}_{t=1}\\left(u_{i}(\\sigma^{*}_{i},\\sigma^{t}_{-i})-u_{i}(\\sigma^{t})\\right)$\n",
    "\n",
    "$i=\\text{player}$ <br>\n",
    "$\\sigma=\\text{player strategy}$ <br>\n",
    "$u=\\text{utitlity function}$ <br>\n",
    "$T=\\text{current time}$\n",
    "\n",
    "## Regret Matching\n",
    "$v_{i}(\\sigma,h)=\\sum_{z \\epsilon Z, h \\sqsubset z} \\pi^{\\sigma}_{-i}(h)\\pi^{\\sigma}_{i}(h,z)u_{i}(z)$\n",
    "\n",
    "$\\pi=\\text{policy at curren time step}$ <br>\n",
    "$h=\\text{current state}$ <br>\n",
    "$u = \\text{utility function at current time step for action}$ <br>\n",
    "$i = \\text{player (negative i refers to the opponent)}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\Delta R^{T}(a)=v^t(a)-\\sum_{b \\epsilon A}{\\sigma^{t}(b)v^{t}(b)}$\n",
    "\n",
    "$v=\\text{counterfactual value}$ <br>\n",
    "$\\sigma = \\text{player policy}$ <br>\n",
    "$t=\\text{timestep}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$R^{t}(a)=R^{t-1}(a)+\\Delta R^{t}(a)$\n",
    "\n",
    "$R=\\text{regret}$ <br>\n",
    "$a=\\text{action}$ <br>\n",
    "$t=\\text{time step}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\sigma^{t}(a)=R^{t-1}(a)^{+}/\\sum_{b \\epsilon A}{R^{t-1}(b)^{+}}$\n",
    "\n",
    "$R=\\text{regret}$ <br>\n",
    "$t=\\text{time step}$ <br>\n",
    "$x^{+}=max(x,0)$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$u^{\\sigma}_{i}=\\sum_{z}\\pi^{\\sigma}(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kuhn poker time\n",
    "# 3 cards\n",
    "# 3*2 = 6 hands\n",
    "\n",
    "HANDS = [(1,2), (1,3), (2,1), (2,3), (3,1), (3,2)]\n",
    "\n",
    "# there are 12 information sets\n",
    "# p1(1st) 1 {2,3}\n",
    "# p1(1st) 2 {1,3}\n",
    "# p1(1st) 3 {1,2}\n",
    "# p2(2nd) 1, p1 pass {2,3}\n",
    "# p2(2nd) 1, p1 bet  {2,3}\n",
    "# p2(2nd) 2, p1 pass {1,3}\n",
    "# p2(2nd) 2, p1 bet  {1,3}\n",
    "# p2(2nd) 3, p1 pass {1,2}\n",
    "# p2(2nd) 3, p1 bet  {1,2}\n",
    "# p1(3rd) 1, p1 pass, p2 bet  {2,3}\n",
    "# p1(3rd) 2, p1 pass, p2 bet  {1,3}\n",
    "# p1(3rd) 3, p1 pass, p2 bet  {1,2}\n",
    "\n",
    "ISETS = [\"1\", \"2\", \"3\",   # round 1\n",
    "         \"P1\", \"P2\", \"P3\", \"B1\", \"B2\", \"B3\",  # round 2\n",
    "         \"PB1\", \"PB2\", \"PB3\"]  # round 3\n",
    "\n",
    "# terminal history states\n",
    "TERMINAL = [\"PP\", \"PBP\", \"PBB\", \"BP\", \"BB\"]\n",
    "ACTIONS = [\"P\", \"B\"]\n",
    "\n",
    "def payout(rs, h):\n",
    "    if h == \"PBP\":\n",
    "        return -1\n",
    "    elif h == \"BP\":\n",
    "        return 1\n",
    "    m = 1 if (rs[0] > rs[1]) else -1\n",
    "    if h == \"PP\":\n",
    "        return m\n",
    "    if h in [\"BB\", \"PBB\"]:\n",
    "        return m*2\n",
    "    assert False\n",
    "\n",
    "def get_information_set(rs, h):\n",
    "    assert h not in TERMINAL\n",
    "    if h == \"\":\n",
    "        return str(rs[0])\n",
    "    elif len(h) == 1:\n",
    "        return h + str(rs[1])\n",
    "    else:\n",
    "        return \"PB\" + str(rs[0])\n",
    "    assert False\n",
    "\n",
    "def cfr(rs, h, i, t, pi1, pi2):\n",
    "    # rs = realstate\n",
    "    # h = history\n",
    "    # i = player\n",
    "    # t = timestep\n",
    "    # pi1 = reach probability player 1\n",
    "    # pi2 = reach probabilty player 2\n",
    "    \n",
    "    if h in TERMINAL:\n",
    "        return payout(rs, h) * (1 if i == 1 else -1)\n",
    "    I = get_information_set(rs, h)\n",
    "    ph = 2 if len(h) == 1 else 1 #Player hand\n",
    "    \n",
    "    # if we are here, we have both actions available\n",
    "    vo = 0.0 #Counter factual value\n",
    "    voa = {}\n",
    "    for a in ACTIONS:\n",
    "        if ph == 1: #Player 1\n",
    "            voa[a] = cfr(rs, h+a, i, t, sigma[t][I][a] * pi1, pi2)\n",
    "        else: #Player 2\n",
    "            voa[a] = cfr(rs, h+a, i, t, pi1, sigma[t][I][a] * pi2)\n",
    "        vo += sigma[t][I][a] * voa[a]\n",
    "    if ph == i:\n",
    "        if i == 1:\n",
    "            pi = pi1\n",
    "            pnegi = pi2\n",
    "        else:\n",
    "            pi = pi2\n",
    "            pnegi = pi1\n",
    "        for a in ACTIONS:\n",
    "            regret[I][a] += pnegi * (voa[a] - vo)\n",
    "            strategy[I][a] += pi * sigma[t][I][a] #Since PI is set @ 1 the strategy is set to current sigma\n",
    "        # update the strategy based on regret\n",
    "        rsum = sum([max(x, 0) for x in regret[I].values()])\n",
    "        for a in ACTIONS:\n",
    "            if rsum > 0:\n",
    "                sigma[t+1][I][a] = max(regret[I][a], 0) / rsum\n",
    "            else:\n",
    "                sigma[t+1][I][a] = 0.5\n",
    "    return vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tables\n",
    "regret = {}\n",
    "strategy = {}\n",
    "for I in ISETS:\n",
    "    regret[I] = {k:0 for k in ACTIONS}\n",
    "    strategy[I] = {k:0 for k in ACTIONS}\n",
    "    \n",
    "sigma = {}\n",
    "sigma[1] = {}\n",
    "for I in ISETS:\n",
    "    sigma[1][I] = {k:0.5 for k in ACTIONS}\n",
    "\n",
    "# learn strategy\n",
    "import copy\n",
    "import random\n",
    "for t in range(1, 1000):\n",
    "    sigma[t+1] = copy.deepcopy(sigma[t])\n",
    "    for i in [1,2]:\n",
    "        cfr(random.choice(HANDS), \"\", i, t, 1, 1)\n",
    "    del sigma[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: P:0.7530 B:0.2470\n",
      "  2: P:0.9665 B:0.0335\n",
      "  3: P:0.3649 B:0.6351\n",
      " P1: P:0.5789 B:0.4211\n",
      " P2: P:0.9941 B:0.0059\n",
      " P3: P:0.0174 B:0.9826\n",
      " B1: P:0.9984 B:0.0016\n",
      " B2: P:0.5653 B:0.4347\n",
      " B3: P:0.0015 B:0.9985\n",
      "PB1: P:0.9991 B:0.0009\n",
      "PB2: P:0.5009 B:0.4991\n",
      "PB3: P:0.0023 B:0.9977\n"
     ]
    }
   ],
   "source": [
    "# print \"average\" strategy\n",
    "for k,v in strategy.items():\n",
    "    norm = sum(list(v.values()))\n",
    "    print(\"%3s: P:%.4f B:%.4f\" % (k, v['P']/norm, v['B']/norm))\n",
    "# https://en.wikipedia.org/wiki/Kuhn_poker#Optimal_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
