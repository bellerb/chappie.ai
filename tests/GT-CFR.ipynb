{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 = $4\n",
      "player 2 = $0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Kuhn_Poker:\n",
    "    def __init__(self, p1 = 2, p2 = 2):\n",
    "        self.pot = 0\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.a1 = None\n",
    "        self.a2 = None\n",
    "        self.actions = ['check','raise','fold']\n",
    "        self.cards = {'king':3, 'queen':2, 'jack':1}\n",
    "        \n",
    "    def deal(self):\n",
    "        cards = list(self.cards.keys())\n",
    "        p1 = random.choice(cards)\n",
    "        cards.remove(p1)\n",
    "        p2 = random.choice(cards)\n",
    "        return p1, p2\n",
    "    \n",
    "    def place_bet(self, player, amount):\n",
    "        self.pot += amount\n",
    "        setattr(self, player, getattr(self, player) - amount)\n",
    "    \n",
    "    def choose_action(self, player, action, amount = 1):\n",
    "        if isinstance(action, int):\n",
    "            setattr(self, f'a{player[-1]}', self.actions[action])\n",
    "        elif str(action).lower() in self.actions:\n",
    "            setattr(self, f'a{player[-1]}', str(action).lower())\n",
    "        else:\n",
    "            return False\n",
    "        if getattr(self, f'a{player[-1]}') == 'raise' or getattr(self, f'a{player[-1]}') == 'call':\n",
    "            game.place_bet(player, amount)\n",
    "        return True\n",
    "    \n",
    "    def pay_out(self, p1, p2):\n",
    "        if self.cards[p1] > self.cards[p2]:\n",
    "            self.p1 += self.pot\n",
    "        else:\n",
    "            self.p2 += self.pot\n",
    "        self.pot = 0\n",
    "        \n",
    "game = Kuhn_Poker()\n",
    "\n",
    "#Anti up\n",
    "game.place_bet('p1', 1)\n",
    "game.place_bet('p2', 1)\n",
    "\n",
    "#Deal cards\n",
    "p1, p2 = game.deal()\n",
    "\n",
    "#Choose action\n",
    "game.choose_action('p1', 'Check')\n",
    "game.choose_action('p2', 'Raise')\n",
    "game.choose_action('p1', 'Call')\n",
    "\n",
    "game.pay_out(p1, p2)\n",
    "print(f'player 1 = ${game.p1}\\nplayer 2 = ${game.p2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash Equilibrium (Aproximation)\n",
    "$u_{1}(\\sigma) + \\epsilon \\geq \\underset{\\sigma^{'}_{1}\\epsilon \\sum_{1}}{\\text{max}}u_{1}(\\sigma^{'}_{1},\\sigma_{2})$ <br>\n",
    "$u_{2}(\\sigma) + \\epsilon \\geq \\underset{\\sigma^{'}_{2}\\epsilon \\sum_{2}}{\\text{max}}u_{2}(\\sigma^{'}_{2},\\sigma_{2})$\n",
    "\n",
    "$u=\\text{utility function}$ <br>\n",
    "$\\sigma=\\text{player strategy}$\n",
    "\n",
    "What this means is that both players are playing to win, meaning they are each playing their best reponces to the players strategy. Because both players are trying to win this makes the best each player can do is tie. Nash equilibrium has an exploitability of zero meaning it is theoretically a unbeatable strategy making it optimal for most games.\n",
    "\n",
    "## Regret-Matching+\n",
    "$r(s,a)=v_{i}(s,a)-\\sum_{b \\epsilon A(s)}{p(s,b)v_{i}(s,b)}$\n",
    "\n",
    "$r=\\text{beliefs (range)}$ <br>\n",
    "$v=\\text{counterfactual value}$ <br>\n",
    "$p=\\text{player policy}$ <br>\n",
    "$s=\\text{public state}$\n",
    "\n",
    "$Q^{t}(s,a)=\\left(Q^{t-1}(s,a)+r^{t}(s,a)\\right)^{+}$\n",
    "\n",
    "$Q=\\text{regret like value}$ <br>\n",
    "$s=\\text{public state}$ <br>\n",
    "$a=\\text{action}$ <br>\n",
    "$r=\\text{beliefs (range)}$ <br>\n",
    "$t=\\text{time step}$\n",
    "\n",
    "$\\pi^{t+1}(s,a)=\\frac{Q^{t}(s,a)}{\\sum_{b}{Q^{t}(s,b)}}$\n",
    "\n",
    "$\\pi=\\text{player strategy}$ <br>\n",
    "$s=\\text{public state}$ <br>\n",
    "$Q=\\text{regret like value}$\n",
    "\n",
    "## Linearly weighted policy averaging\n",
    "\n",
    "$\\bar{\\pi}^{T}_{i}=\\frac{2}{(T^{2}+T)\\sum^{T}_{t=1}{t\\pi^{t}_{i}}}$\n",
    "\n",
    "$\\pi=\\text{player strategy}$ <br>\n",
    "$i=\\text{player}$ <br>\n",
    "$T=\\text{Iterations}$\n",
    "\n",
    "## pUCT\n",
    "\n",
    "$pUCT=Q(s,a)+P(s,a) \\cdot \\frac{\\sqrt{\\sum_{b}{N(s,b)}}}{1+N(s,a)}\\left(c_{1}+log\\left(\\frac{\\sum_{b}{N(s,b)+c_{2}+1}}{c_{2}}\\right)\\right)$\n",
    "\n",
    "$Q=\\text{counterfactual value}$ <br>\n",
    "$P=\\text{policy}$ <br>\n",
    "$N=\\text{visit count}$ <br>\n",
    "$c_{1}=\\text{exploration hyperparameter}$ <br>\n",
    "$c_{2}=\\text{exploration hyperparameter}$ <br>\n",
    "$s=\\text{current state}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from copy import deepcopy\n",
    "\n",
    "class GT_CFR:\n",
    "    def __init__(self, nn, s, c, action_space, k = 1, c1 = 1.25, c2 = 19652):\n",
    "        self.T = 0\n",
    "        self.c1 = c1 #Hyperparameter\n",
    "        self.c2 = c2 #Hyperparameter\n",
    "        self.k = k #Total actions to expand\n",
    "        self.s = s #Total expansion simulations\n",
    "        self.c = c #Expansion simulations per regret update\n",
    "        self.H = [] #History\n",
    "        self.L = {} #Tree\n",
    "        self.action_space = action_space #Total actions\n",
    "        self.CVPN = nn\n",
    "        \n",
    "    class Node:\n",
    "        def __init__(self):\n",
    "            self.N = 0\n",
    "            self.p_sum = 0\n",
    "            self.Q = []\n",
    "            self.v = []\n",
    "            self.p = []\n",
    "            self.r = []\n",
    "            self.actions = []\n",
    "        \n",
    "    def state_hash(self, s):\n",
    "        result = hash(str(s))\n",
    "        return result\n",
    "    \n",
    "    def regret_matching_plus(self, Q, Q_sum):\n",
    "        return Q / Q_sum if Q_sum > 0 else 0\n",
    "    \n",
    "    def pUCT(self, c, p_hash, idx):\n",
    "        c_hash = self.state_hash(c)\n",
    "        U = log(sum([self.L[self.state_hash(a)].N + self.c2 + 1 \n",
    "                     for a in self.L[p_hash].actions]) / self.c2)\n",
    "        U += self.c1\n",
    "        U *= self.L[p_hash].p[idx]\n",
    "        U *= self.L[p_hash].N / (1 + self.L[c_hash].N)\n",
    "        U += self.L[p_hash].v[idx]\n",
    "        return U\n",
    "    \n",
    "    def CFR_plus(self, s, Q_1 = 0):\n",
    "        s_hash = self.state_hash(s)\n",
    "        \n",
    "        #Use CVPN if leaf node\n",
    "        if len(self.L[s_hash].actions) == 0: \n",
    "            B = (s, self.L[s_hash].r)\n",
    "            v, p = self.CVPN.predict(B)\n",
    "            self.L[s_hash].v = v\n",
    "            self.L[s_hash].p = p\n",
    "\n",
    "        #Update regret like values\n",
    "        r_sum = sum([v * p for v, p in zip(self.L[s_hash].v, self.L[s_hash].p)])\n",
    "        for i, v in enumerate(self.L[s_hash].v):\n",
    "            r = v - r_sum #Regret\n",
    "            self.L[s_hash].r.append(r)\n",
    "            if len(self.L[s_hash].Q) == len(self.L[s_hash].v):\n",
    "                self.L[s_hash].Q[i] = max(Q_1 + r, 0)\n",
    "            else:\n",
    "                self.L[s_hash].Q.append(max(Q_1 + r, 0))\n",
    "        \n",
    "        #Update policy & go to next node\n",
    "        if len(self.L[s_hash].actions) > 0:\n",
    "            Q_sum = sum(self.L[s_hash].Q)\n",
    "            for i, Q in enumerate(self.L[s_hash].Q):\n",
    "                #Regret matching +\n",
    "                pi = self.regret_matching_plus(Q, Q_sum)\n",
    "                #Linearly weighted policy average\n",
    "                self.L[s_hash].p_sum += pi * self.T\n",
    "                pi = 2 / (((self.T ** 2) + self.T) * self.L[s_hash].p_sum) if pi != 0 else 0\n",
    "                self.L[s_hash].p[i] = pi\n",
    "                #Go to next node\n",
    "                self.CFR_plus(self.L[s_hash].actions[i], Q_1 = Q)\n",
    "    \n",
    "    def expansion(self, s):\n",
    "        s_hash = self.state_hash(s)\n",
    "        if len(self.L[s_hash].actions) > 0:\n",
    "            #Traverse tree\n",
    "            pi_bank = []\n",
    "            Q_sum = sum(self.L[s_hash].Q)\n",
    "            for i, Q in enumerate(self.L[s_hash].Q):\n",
    "                rm_plus = self.regret_matching_plus(Q, Q_sum)\n",
    "                pUCT = self.pUCT(self.L[s_hash].actions[i], s_hash, i)\n",
    "                pi = (.5 * rm_plus) + (.5 * pUCT)\n",
    "                pi_bank.append(pi)\n",
    "            m_pi = max(pi_bank)\n",
    "            a_bank = [i for i, v in enumerate(pi_bank) if v == m_pi]\n",
    "            a = random.choice(a_bank)\n",
    "            self.expansion(self.L[s_hash].actions[a])\n",
    "        else:\n",
    "            #Add action to tree\n",
    "            for a in range(self.action_space):\n",
    "                s_1 = self.perform_action(s, a)\n",
    "                search.L[s_hash].actions.append(s_1)\n",
    "                a_hash = self.state_hash(s_1)\n",
    "                search.L[a_hash] = search.Node()\n",
    "        self.L[s_hash].N += 1 #Update node visit count\n",
    "    \n",
    "    def GT_CFR(self, I):\n",
    "        for t in range(int(self.s / self.c)):\n",
    "            self.T = t + 1\n",
    "            self.CFR_plus(I) #Run CFR\n",
    "            for _ in range(self.c):\n",
    "                self.expansion(I) #Grow tree\n",
    "                \n",
    "    def perform_action(self, s, a):\n",
    "        s_1 = deepcopy(s)\n",
    "        if a == 1 and s_1[0] == 1:\n",
    "            s_1[-2] += 1\n",
    "        elif a == 2 and s_1[0] == 1:\n",
    "            s_1[-1] += s_1[-2]\n",
    "            s_1[-2] = 0\n",
    "        elif (a == 0 or a == 1) and s_1[0] == -1:\n",
    "            s_1[2] = 3 if s_1[1] == 2 else 2\n",
    "        elif a == 1:\n",
    "            s_1[-1] += 1\n",
    "        elif a == 2:\n",
    "            s_1[-2] += s_1[-1]\n",
    "            s_1[-1] = 0\n",
    "        s_1[0] *= -1\n",
    "        return s_1\n",
    "    \n",
    "class CVPN:\n",
    "    def predict(self, B):\n",
    "        s, r = B\n",
    "        c = max(s)\n",
    "        if c == 3:\n",
    "            v = [0.5, 0.5, 0]\n",
    "            p = [0.25, 0.25, 0.25]\n",
    "        elif c == 2:\n",
    "            v = [0.25, 0.25, 0.25]\n",
    "            p = [0.25, 0.25, 0.25]\n",
    "        else:\n",
    "            v = [0.5, 0, 0.5]\n",
    "            p = [0.25, 0.25, 0.25]\n",
    "        return v, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king queen\n",
      "[[-1, 3, -1, 1, 1], [-1, 3, -1, 2, 1], [-1, 3, -1, 0, 2]]\n",
      "[0.25, 0.25, 0]\n"
     ]
    }
   ],
   "source": [
    "nn = CVPN()\n",
    "search = GT_CFR(nn, 2, 1, 3)\n",
    "\n",
    "game = Kuhn_Poker()\n",
    "\n",
    "#Anti up\n",
    "game.place_bet('p1', 1)\n",
    "game.place_bet('p2', 1)\n",
    "\n",
    "#Deal cards\n",
    "p1, p2 = game.deal()\n",
    "p1 = 'king'\n",
    "p2 = 'queen'\n",
    "\n",
    "print(p1, p2)\n",
    "s = [1] + [game.cards[p1]] + [-1, 1, 1]\n",
    "s_hash = search.state_hash(s)\n",
    "search.L[s_hash] = search.Node()\n",
    "search.L[s_hash].r = [1/(len(game.cards) - 1) if c != p1 else 0 for c in game.cards]\n",
    "\n",
    "search.GT_CFR(s)\n",
    "\n",
    "print(search.L[s_hash].actions)\n",
    "print(search.L[s_hash].Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average overall Regret\n",
    "$R^{T}_{i}=\\frac{1}{T} \\underset{\\sigma^{*}_{i}\\epsilon \\sum_{i}}{\\text{max}} \\sum^{T}_{t=1}\\left(u_{i}(\\sigma^{*}_{i},\\sigma^{t}_{-i})-u_{i}(\\sigma^{t})\\right)$\n",
    "\n",
    "$i=\\text{player}$ <br>\n",
    "$\\sigma=\\text{player strategy}$ <br>\n",
    "$u=\\text{utitlity function}$ <br>\n",
    "$T=\\text{current time}$\n",
    "\n",
    "## Regret Matching\n",
    "$v_{i}(\\sigma,h)=\\sum_{z \\epsilon Z, h \\sqsubset z} \\pi^{\\sigma}_{-i}(h)\\pi^{\\sigma}_{i}(h,z)u_{i}(z)$\n",
    "\n",
    "$\\pi=\\text{policy at curren time step}$ <br>\n",
    "$h=\\text{current state}$ <br>\n",
    "$u = \\text{utility function at current time step for action}$ <br>\n",
    "$i = \\text{player (negative i refers to the opponent)}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\Delta R^{T}(a)=v^t(a)-\\sum_{b \\epsilon A}{\\sigma^{t}(b)v^{t}(b)}$\n",
    "\n",
    "$v=\\text{counterfactual value}$ <br>\n",
    "$\\sigma = \\text{player policy}$ <br>\n",
    "$t=\\text{timestep}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$R^{t}(a)=R^{t-1}(a)+\\Delta R^{t}(a)$\n",
    "\n",
    "$R=\\text{regret}$ <br>\n",
    "$a=\\text{action}$ <br>\n",
    "$t=\\text{time step}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$\\sigma^{t}(a)=R^{t-1}(a)^{+}/\\sum_{b \\epsilon A}{R^{t-1}(b)^{+}}$\n",
    "\n",
    "$R=\\text{regret}$ <br>\n",
    "$t=\\text{time step}$ <br>\n",
    "$x^{+}=max(x,0)$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$u^{\\sigma}_{i}=\\sum_{z}\\pi^{\\sigma}(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kuhn poker time\n",
    "# 3 cards\n",
    "# 3*2 = 6 hands\n",
    "\n",
    "HANDS = [(1,2), (1,3), (2,1), (2,3), (3,1), (3,2)]\n",
    "\n",
    "# there are 12 information sets\n",
    "# p1(1st) 1 {2,3}\n",
    "# p1(1st) 2 {1,3}\n",
    "# p1(1st) 3 {1,2}\n",
    "# p2(2nd) 1, p1 pass {2,3}\n",
    "# p2(2nd) 1, p1 bet  {2,3}\n",
    "# p2(2nd) 2, p1 pass {1,3}\n",
    "# p2(2nd) 2, p1 bet  {1,3}\n",
    "# p2(2nd) 3, p1 pass {1,2}\n",
    "# p2(2nd) 3, p1 bet  {1,2}\n",
    "# p1(3rd) 1, p1 pass, p2 bet  {2,3}\n",
    "# p1(3rd) 2, p1 pass, p2 bet  {1,3}\n",
    "# p1(3rd) 3, p1 pass, p2 bet  {1,2}\n",
    "\n",
    "ISETS = [\"1\", \"2\", \"3\",   # round 1\n",
    "         \"P1\", \"P2\", \"P3\", \"B1\", \"B2\", \"B3\",  # round 2\n",
    "         \"PB1\", \"PB2\", \"PB3\"]  # round 3\n",
    "\n",
    "# terminal history states\n",
    "TERMINAL = [\"PP\", \"PBP\", \"PBB\", \"BP\", \"BB\"]\n",
    "ACTIONS = [\"P\", \"B\"]\n",
    "\n",
    "def payout(rs, h):\n",
    "    if h == \"PBP\":\n",
    "        return -1\n",
    "    elif h == \"BP\":\n",
    "        return 1\n",
    "    m = 1 if (rs[0] > rs[1]) else -1\n",
    "    if h == \"PP\":\n",
    "        return m\n",
    "    if h in [\"BB\", \"PBB\"]:\n",
    "        return m*2\n",
    "    assert False\n",
    "\n",
    "def get_information_set(rs, h):\n",
    "    assert h not in TERMINAL\n",
    "    if h == \"\":\n",
    "        return str(rs[0])\n",
    "    elif len(h) == 1:\n",
    "        return h + str(rs[1])\n",
    "    else:\n",
    "        return \"PB\" + str(rs[0])\n",
    "    assert False\n",
    "\n",
    "def cfr(rs, h, i, t, pi1, pi2):\n",
    "    # rs = realstate\n",
    "    # h = history\n",
    "    # i = player\n",
    "    # t = timestep\n",
    "    # pi1 = reach probability player 1\n",
    "    # pi2 = reach probabilty player 2\n",
    "    \n",
    "    if h in TERMINAL:\n",
    "        return payout(rs, h) * (1 if i == 1 else -1)\n",
    "    I = get_information_set(rs, h)\n",
    "    ph = 2 if len(h) == 1 else 1 #Player hand\n",
    "    \n",
    "    # if we are here, we have both actions available\n",
    "    vo = 0.0 #Counter factual value\n",
    "    voa = {}\n",
    "    for a in ACTIONS:\n",
    "        if ph == 1: #Player 1\n",
    "            voa[a] = cfr(rs, h+a, i, t, sigma[t][I][a] * pi1, pi2)\n",
    "        else: #Player 2\n",
    "            voa[a] = cfr(rs, h+a, i, t, pi1, sigma[t][I][a] * pi2)\n",
    "        vo += sigma[t][I][a] * voa[a]\n",
    "    if ph == i:\n",
    "        if i == 1:\n",
    "            pi = pi1\n",
    "            pnegi = pi2\n",
    "        else:\n",
    "            pi = pi2\n",
    "            pnegi = pi1\n",
    "        for a in ACTIONS:\n",
    "            regret[I][a] += pnegi * (voa[a] - vo)\n",
    "            strategy[I][a] += pi * sigma[t][I][a] #Since PI is set @ 1 the strategy is set to current sigma\n",
    "        # update the strategy based on regret\n",
    "        rsum = sum([max(x, 0) for x in regret[I].values()])\n",
    "        for a in ACTIONS:\n",
    "            if rsum > 0:\n",
    "                sigma[t+1][I][a] = max(regret[I][a], 0) / rsum\n",
    "            else:\n",
    "                sigma[t+1][I][a] = 0.5\n",
    "    return vo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tables\n",
    "regret = {}\n",
    "strategy = {}\n",
    "for I in ISETS:\n",
    "    regret[I] = {k:0 for k in ACTIONS}\n",
    "    strategy[I] = {k:0 for k in ACTIONS}\n",
    "    \n",
    "sigma = {}\n",
    "sigma[1] = {}\n",
    "for I in ISETS:\n",
    "    sigma[1][I] = {k:0.5 for k in ACTIONS}\n",
    "\n",
    "# learn strategy\n",
    "import copy\n",
    "import random\n",
    "for t in range(1, 1000):\n",
    "    sigma[t+1] = copy.deepcopy(sigma[t])\n",
    "    for i in [1,2]:\n",
    "        cfr(random.choice(HANDS), \"\", i, t, 1, 1)\n",
    "    del sigma[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: P:0.7443 B:0.2557\n",
      "  2: P:0.9943 B:0.0057\n",
      "  3: P:0.3312 B:0.6688\n",
      " P1: P:0.6164 B:0.3836\n",
      " P2: P:0.9938 B:0.0063\n",
      " P3: P:0.0015 B:0.9985\n",
      " B1: P:0.9985 B:0.0015\n",
      " B2: P:0.5886 B:0.4114\n",
      " B3: P:0.0015 B:0.9985\n",
      "PB1: P:0.9990 B:0.0010\n",
      "PB2: P:0.5094 B:0.4906\n",
      "PB3: P:0.0024 B:0.9976\n"
     ]
    }
   ],
   "source": [
    "# print \"average\" strategy\n",
    "for k,v in strategy.items():\n",
    "    norm = sum(list(v.values()))\n",
    "    print(\"%3s: P:%.4f B:%.4f\" % (k, v['P']/norm, v['B']/norm))\n",
    "# https://en.wikipedia.org/wiki/Kuhn_poker#Optimal_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
